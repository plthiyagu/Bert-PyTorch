{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQY9c8J/SXNNCDwn4R87Rw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramkumarr02/Bert-PyTorch/blob/master/BERT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmf6YAZyUBlk",
        "colab_type": "text"
      },
      "source": [
        "# Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n2_aiUQUE0S",
        "colab_type": "text"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkp4cSGW1uu",
        "colab_type": "code",
        "outputId": "054a81f1-d501-42e3-f2cb-7111e113453a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBDDNnlWuKE",
        "colab_type": "code",
        "outputId": "f61a09bb-4550-46a8-ffa7-0d9bc4689097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Rb1TtSNTkQ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive and Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIgyszSkUGv_",
        "colab_type": "code",
        "outputId": "8e592f54-a897-4258-e75e-d11a1cb03e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Deep Learning/Language Models/BERT/IMDB Dataset.csv')\n",
        "df_copy = df.copy()\n",
        "df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evg3lWacNgmt",
        "colab_type": "text"
      },
      "source": [
        "# Classes & Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQctr7OPNaCu",
        "colab_type": "text"
      },
      "source": [
        "## Data Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJBmCoFqinL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bert_data_prep:\n",
        "    def __init__(self,x, y, tokenizer, max_len = 250):\n",
        "        self.x = x.values\n",
        "        self.y = y.values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.x))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        x = self.x[item]\n",
        "    \n",
        "        x_encoded = tokenizer.encode_plus(x,\n",
        "                                        add_special_tokens=True, \n",
        "                                        max_length = max_len, \n",
        "                                        pad_to_max_length = True, \n",
        "                                        return_tensors = 'pt'\n",
        "                                        )  \n",
        "        \n",
        "        ids = x_encoded[\"input_ids\"]\n",
        "        mask = x_encoded[\"attention_mask\"]\n",
        "        token_type_ids = x_encoded[\"token_type_ids\"]\n",
        "        targets = torch.tensor(train_y, dtype=torch.float)\n",
        "\n",
        "        ids             = torch.squeeze(ids,0)\n",
        "        mask            = torch.squeeze(mask,0)\n",
        "        token_type_ids  = torch.squeeze(token_type_ids,0)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.y[item], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dDmeLKaNor9",
        "colab_type": "text"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "439d568b-e160-4fce-bec9-8be5abd19b1e",
        "id": "Y40tQJWoG3Vl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''x = train_x.values\n",
        "x = x[1]'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x = train_x.values\\nx = x[1]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiGw8yQsClRV",
        "colab_type": "code",
        "outputId": "1cfeaacc-2258-454d-e14b-d9e240b392e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''x_encoded = tokenizer.encode_plus(x,\n",
        "                                    add_special_tokens=True, \n",
        "                                    max_length = max_len, \n",
        "                                    pad_to_max_length = True, \n",
        "                                    return_tensors = 'pt'\n",
        "                                    )  \n",
        "    \n",
        "ids = x_encoded[\"input_ids\"]\n",
        "mask = x_encoded[\"attention_mask\"]\n",
        "token_type_ids = x_encoded[\"token_type_ids\"]\n",
        "targets = torch.tensor(train_y, dtype=torch.float)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x_encoded = tokenizer.encode_plus(x,\\n                                    add_special_tokens=True, \\n                                    max_length = max_len, \\n                                    pad_to_max_length = True, \\n                                    return_tensors = \\'pt\\'\\n                                    )  \\n    \\nids = x_encoded[\"input_ids\"]\\nmask = x_encoded[\"attention_mask\"]\\ntoken_type_ids = x_encoded[\"token_type_ids\"]\\ntargets = torch.tensor(train_y, dtype=torch.float)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCiLgY9XVDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.sentiment = df.sentiment.replace(['positive', 'negative'],[1,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyjHs-GR0Vb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = train_test_split(df['review'], df['sentiment'],train_size = 0.8,random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HSARV8CNaBH",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuk_BWanOhoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_str = max(df.review, key = len)\n",
        "max_str_index = list(df.review).index(max_str)\n",
        "max_len = len(df.review[max_str_index].split())\n",
        "max_len = 64\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyk_oVkrxJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = Bert_data_prep(x = train_x, y = train_y, tokenizer = tokenizer, max_len = max_len)\n",
        "valid_df = Bert_data_prep(x = valid_x, y = valid_y, tokenizer = tokenizer, max_len = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ4tw_vDN5Cz",
        "colab_type": "text"
      },
      "source": [
        "## Data *Loader*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zaadfHTcG9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_df, batch_size=train_batch_size)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_df, batch_size=valid_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_M-dDFTR9Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "\n",
        "optimizer = AdamW(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggpPntWhR9H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer= optimizer, num_warmup_steps=0, num_training_steps= total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtAt7xXRR890",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5UVVfpCU32A",
        "colab_type": "code",
        "outputId": "28bee853-89f0-419a-f34b-bc08d715ed18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss_list = []\n",
        "accuracy_list = []\n",
        "for epoch in tqdm(range(epochs)):    \n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_data_loader):\n",
        "        ids             = batch['ids']\n",
        "        mask            = batch['mask']\n",
        "        targets         = batch['targets']\n",
        "        token_type_ids  = batch['token_type_ids']             \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, token_type_ids = token_type_ids, labels = targets)\n",
        "        loss = outputs[0]\n",
        "        print(loss)\n",
        "        break    \n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8023, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4DoFQLuw9Eh",
        "colab_type": "code",
        "outputId": "9dce79d7-b74d-4ad6-c93b-c69ac741934a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "l = [ids, mask, token_type_ids, targets]\n",
        "\n",
        "for i in l:\n",
        "    print((i[1].dtype))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.int64\n",
            "torch.int64\n",
            "torch.int64\n",
            "torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2aaGuspHm8m",
        "colab_type": "code",
        "outputId": "f2eae8dd-6264-49a3-af1b-f2eeec03405e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ids.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5CPHzwBE1Zi",
        "colab_type": "code",
        "outputId": "f9066f0e-1c11-4712-e29d-91af9095c1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.squeeze(ids,0).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyoT2RoFDD6d",
        "colab_type": "code",
        "outputId": "c22e1b1d-34d4-427f-f891-b167464320bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i, batch in enumerate(train_data_loader):\n",
        "    ids             = batch['ids']\n",
        "    mask            = batch['mask']\n",
        "    targets         = batch['targets']\n",
        "    token_type_ids  = batch['token_type_ids']\n",
        "\n",
        "    print(ids.shape)\n",
        "    print(mask.shape)    \n",
        "    print(token_type_ids.shape)\n",
        "    print(targets.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1, 64])\n",
            "torch.Size([32, 1, 64])\n",
            "torch.Size([32, 1, 64])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GHYQMXVAW-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
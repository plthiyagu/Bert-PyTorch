{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5vFF0rOTlv25l4T2kVHX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramkumarr02/Bert-PyTorch/blob/master/BERT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmf6YAZyUBlk",
        "colab_type": "text"
      },
      "source": [
        "# Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n2_aiUQUE0S",
        "colab_type": "text"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkp4cSGW1uu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "79d8296d-2106-4922-afc7-9f48615a02c4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBDDNnlWuKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertModel\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Rb1TtSNTkQ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive and Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIgyszSkUGv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "23bc4819-a043-4227-ceee-fd54a21fc75f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Deep Learning/Language Models/BERT/IMDB Dataset.csv')\n",
        "df_copy = df.copy()\n",
        "df.head(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evg3lWacNgmt",
        "colab_type": "text"
      },
      "source": [
        "# Classes & Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQctr7OPNaCu",
        "colab_type": "text"
      },
      "source": [
        "## Data Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJBmCoFqinL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bert_data_prep:\n",
        "    def __init__(self,x, y, tokenizer, max_len = 250):\n",
        "        self.x = x.values\n",
        "        self.y = y.values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.x))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        x = self.x[item]\n",
        "    \n",
        "        x_encoded = tokenizer.encode_plus(x,\n",
        "                                        add_special_tokens=True, \n",
        "                                        max_length = max_len, \n",
        "                                        pad_to_max_length = True, \n",
        "                                        return_tensors = 'pt'\n",
        "                                        )  \n",
        "        \n",
        "        ids = x_encoded[\"input_ids\"]\n",
        "        mask = x_encoded[\"attention_mask\"]\n",
        "        token_type_ids = x_encoded[\"token_type_ids\"]\n",
        "\n",
        "        ids             = torch.squeeze(ids,0)\n",
        "        mask            = torch.squeeze(mask,0)\n",
        "        token_type_ids  = torch.squeeze(token_type_ids,0)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.y[item], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWPACLraY2MA",
        "colab_type": "text"
      },
      "source": [
        "## Test Data Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQN7vUpTQS4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class test_data_prep:\n",
        "    def __init__(self, x, tokenizer, max_len = 250):\n",
        "        self.x = x.values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.x))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        x = self.x[item]\n",
        "    \n",
        "        x_encoded = tokenizer.encode_plus(x,\n",
        "                                        add_special_tokens=True, \n",
        "                                        max_length = max_len, \n",
        "                                        pad_to_max_length = True, \n",
        "                                        return_tensors = 'pt'\n",
        "                                        )  \n",
        "        \n",
        "        ids = x_encoded[\"input_ids\"]\n",
        "        mask = x_encoded[\"attention_mask\"]\n",
        "        token_type_ids = x_encoded[\"token_type_ids\"]\n",
        "\n",
        "        ids             = torch.squeeze(ids,0)\n",
        "        mask            = torch.squeeze(mask,0)\n",
        "        token_type_ids  = torch.squeeze(token_type_ids,0)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv1GfRZfnZpm",
        "colab_type": "text"
      },
      "source": [
        "## Custom BERT Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWZdf8wUUYF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        bo = self.bert_drop(o2)\n",
        "        output = self.out(bo)\n",
        "        return(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4usKEhLnmHT",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtAt7xXRR890",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(outs, targs):\n",
        "\n",
        "    outs = torch.sigmoid(outs).cpu().detach().numpy().tolist()\n",
        "    targs= targs.cpu().detach().numpy().tolist()\n",
        "    outs = np.array(outs) >= 0.5\n",
        "    acc = metrics.accuracy_score(targs, outs)\n",
        "\n",
        "    return(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AKeitxW0ep2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    #preds = preds.detach().cpu().numpy()\n",
        "    preds = preds.detach().to('cpu').numpy()\n",
        "    labels = labels.to('cpu').numpy()\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ialVWpQyn60I",
        "colab_type": "text"
      },
      "source": [
        "# Code Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dDmeLKaNor9",
        "colab_type": "text"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCiLgY9XVDa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd178a75-8244-4b53-d2a4-181f85e0c2b2"
      },
      "source": [
        "df.sentiment = df.sentiment.replace(['positive', 'negative'],[1,0])\n",
        "print(len(df))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DigDqeFSDjV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sample(n=2000, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyjHs-GR0Vb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = train_test_split(df['review'], df['sentiment'],train_size = 0.8,random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2i_oA_9oEKI",
        "colab_type": "text"
      },
      "source": [
        "## Find Max Batch size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSo3VdLRUcME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2051787b-13a6-407c-fc4e-a28082b6b218"
      },
      "source": [
        "doc = [len(train_x), len(valid_x)]\n",
        "\n",
        "train_len = (len(train_x))\n",
        "valid_len = (len(valid_x))\n",
        "\n",
        "train_list = []\n",
        "valid_list = []\n",
        "\n",
        "for divisor in range(1,11):\n",
        "    if train_len % 2**divisor == 0 & train_len % 2 == 0:\n",
        "        train_list.append(divisor)\n",
        "\n",
        "for divisor in range(1,11):\n",
        "    if valid_len % 2**divisor == 0 & valid_len % 2 == 0:\n",
        "        valid_list.append(divisor)\n",
        "\n",
        "max_power = max(set(train_list).intersection(valid_list))\n",
        "max_power"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HSARV8CNaBH",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuk_BWanOhoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1ce65d6-36b6-4458-f150-190c0a1a0cc8"
      },
      "source": [
        "max_str = max(df.review, key = len)\n",
        "max_str_index = list(df.review).index(max_str)\n",
        "#max_len = len(df.review[max_str_index].split())\n",
        "max_len = 256\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "#power = max_power\n",
        "power = 3\n",
        "\n",
        "train_batch_size = 2**power\n",
        "valid_batch_size = 2**power\n",
        "\n",
        "print(f'train_batch_size : {train_batch_size}')\n",
        "print(f'valid_batch_size : {valid_batch_size}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_batch_size : 8\n",
            "valid_batch_size : 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nl1GHGZozLc",
        "colab_type": "text"
      },
      "source": [
        "## Convert to Bert Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyk_oVkrxJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = Bert_data_prep(x = train_x, y = train_y, tokenizer = tokenizer, max_len = max_len)\n",
        "valid_df = Bert_data_prep(x = valid_x, y = valid_y, tokenizer = tokenizer, max_len = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ4tw_vDN5Cz",
        "colab_type": "text"
      },
      "source": [
        "## Data *Loader*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zaadfHTcG9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_df, batch_size=train_batch_size)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_df, batch_size=valid_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNJBTWu2zWcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTBaseUncased()\n",
        "model.cuda()\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggpPntWhR9H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer= optimizer, num_warmup_steps=0, num_training_steps= total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CdUatL76DcT",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqM9XOvM6BkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obzGz4rA_9a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nl = '\\n'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toq2v6FYaGoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "bafc9c59-227d-4046-b582-b131d1b9f949"
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "loss_list = []\n",
        "accuracy_list = []\n",
        "for epoch in tqdm(range(epochs)):    \n",
        "    model.train()\n",
        "    for i, batch_train in enumerate(train_data_loader):\n",
        "        ids             = batch_train['ids']\n",
        "        mask            = batch_train['mask']\n",
        "        targets         = batch_train['targets'].type(torch.FloatTensor)\n",
        "        token_type_ids  = batch_train['token_type_ids']           \n",
        "\n",
        "        ids             = ids.to(device, dtype = torch.long)\n",
        "        targets         = targets.to(device, dtype = torch.float)\n",
        "        mask            = mask.to(device, dtype = torch.long)\n",
        "        token_type_ids  = token_type_ids.to(device, dtype = torch.long) \n",
        "        #token_type_ids = None\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n",
        "        loss = nn.BCEWithLogitsLoss()(train_outputs, targets.view(-1, 1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()    \n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for j, batch_valid in enumerate(valid_data_loader):\n",
        "            ids             = batch_valid['ids']\n",
        "            mask            = batch_valid['mask']\n",
        "            targets         = batch_valid['targets'].type(torch.FloatTensor)\n",
        "            token_type_ids  = batch_valid['token_type_ids']\n",
        "\n",
        "            ids             = ids.to(device, dtype = torch.long)\n",
        "            targets         = targets.to(device, dtype = torch.float)\n",
        "            mask            = mask.to(device, dtype = torch.long)\n",
        "            token_type_ids  = token_type_ids.to(device, dtype = torch.long) \n",
        "            #token_type_ids = None\n",
        "\n",
        "            outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids) \n",
        "\n",
        "            \n",
        "        loss_list.append(loss.data)\n",
        "\n",
        "        #outs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n",
        "        #targs= targets.cpu().detach().numpy().tolist()\n",
        "        print(outputs)\n",
        "        accuracy_val = accuracy(outputs, targets)        \n",
        "\n",
        "        #accuracy = flat_accuracy(outputs, targets)\n",
        "            \n",
        "        accuracy_list.append(accuracy_val)\n",
        "\n",
        "        #print('\\n',outputs)\n",
        "        print(f'{nl}Epoch : {epoch}, Loss : {loss.data} , Accuracy : {accuracy_val}') \n",
        "\n",
        "end = datetime.now()\n",
        "time_elapsed = relativedelta(end, start)\n",
        "print(f'{nl} time_elapsed : {time_elapsed}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 1/4 [00:52<02:38, 52.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.1795],\n",
            "        [ 3.3373],\n",
            "        [-3.4162],\n",
            "        [ 2.6159],\n",
            "        [-3.2857],\n",
            "        [ 1.1427],\n",
            "        [ 3.3986],\n",
            "        [ 2.7590]], device='cuda:0')\n",
            "\n",
            "Epoch : 0, Loss : 0.22424104809761047 , Accuracy : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 2/4 [01:45<01:45, 52.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-4.1975],\n",
            "        [ 4.5461],\n",
            "        [-4.6061],\n",
            "        [ 5.0110],\n",
            "        [-4.3377],\n",
            "        [ 1.9100],\n",
            "        [ 4.9645],\n",
            "        [ 4.0220]], device='cuda:0')\n",
            "\n",
            "Epoch : 1, Loss : 0.0781005322933197 , Accuracy : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 3/4 [02:38<00:52, 52.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.0675],\n",
            "        [ 5.3890],\n",
            "        [-5.3279],\n",
            "        [ 5.5838],\n",
            "        [-5.0936],\n",
            "        [ 0.0412],\n",
            "        [ 5.4964],\n",
            "        [ 4.8824]], device='cuda:0')\n",
            "\n",
            "Epoch : 2, Loss : 0.026963582262396812 , Accuracy : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [03:31<00:00, 52.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.0410],\n",
            "        [ 5.5050],\n",
            "        [-5.5293],\n",
            "        [ 5.7374],\n",
            "        [-5.2366],\n",
            "        [ 1.6677],\n",
            "        [ 5.7237],\n",
            "        [ 5.0632]], device='cuda:0')\n",
            "\n",
            "Epoch : 3, Loss : 0.034449417144060135 , Accuracy : 1.0\n",
            "\n",
            " time_elapsed : relativedelta(minutes=+3, seconds=+31, microseconds=+183155)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXgtcREIZ5ew",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y489kvD2M6Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_list = ['That is good',\n",
        "             'It could have been better',\n",
        "             'Not bad',\n",
        "             'Not good',\n",
        "             'The story was confusing but engaging',\n",
        "             'The story was convoluted and confusing']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuSpyCJnZWxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.DataFrame(test_list, columns = ['comments'])\n",
        "batch_size = len(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EusqEcAQRhO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test_data_prep(x = test_df['comments'], tokenizer = tokenizer, max_len = max_len)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVjyRojASO9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " for j, batch_valid in enumerate(test_data_loader):\n",
        "            ids             = batch_valid['ids']\n",
        "            mask            = batch_valid['mask']\n",
        "            #targets         = batch_valid['targets'].type(torch.FloatTensor)\n",
        "            token_type_ids  = batch_valid['token_type_ids']\n",
        "\n",
        "            ids             = ids.to(device, dtype = torch.long)\n",
        "            #targets         = targets.to(device, dtype = torch.float)\n",
        "            mask            = mask.to(device, dtype = torch.long)\n",
        "            token_type_ids  = token_type_ids.to(device, dtype = torch.long)            \n",
        "\n",
        "            outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids) \n",
        "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3mUUZQxYAxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = ['Positive' if x[0] > 0.5 else 'Neg' for x in outputs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_gPM0RqYHF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "fd6658e8-b47b-4e3a-89ff-7edc79cb3cd9"
      },
      "source": [
        "test_df['results'] = results\n",
        "test_df"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That is good</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It could have been better</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not bad</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Not good</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The story was confusing but engaging</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The story was convoluted and confusing</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 comments   results\n",
              "0                            That is good  Positive\n",
              "1               It could have been better       Neg\n",
              "2                                 Not bad       Neg\n",
              "3                                Not good       Neg\n",
              "4    The story was confusing but engaging  Positive\n",
              "5  The story was convoluted and confusing       Neg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    }
  ]
}